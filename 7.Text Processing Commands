Text processing in Linux involves manipulating text data to extract meaningful information or to transform it into a desired output. 
This is particularly useful because in Linux, everything is treated as a file, including text data. 
By mastering Linux text processing commands, you can complete tasks more efficiently, saving time and effort in your daily workflow.

These commands are especially useful in several scenarios, such as when working with log files during debugging, in CI/CD pipelines, or when handling large amounts of data.

A Brief Overview Of STDIN, STDOUT & STDERR

Standard input (stdin), standard output (stdout), and standard error (stderr) are the three standard streams that allow users to interact with commands and scripts in Linux. 
Understanding how to use these streams is crucial for efficiently working with Linux commands and scripts. 
Without a clear understanding of stdin, stdout, and stderr, it can be challenging to effectively manage command output.

Stdin - short for Standard input, It’s the way you give input to a program. For example:

grep something file.txt
The command above reads the content of the file, search for a pattern, then prints the result of the search in your shell. 
In this case, the content of file.txt is stdin. 

Stdout - Short for Standard output, It’s an output stream that a program writes its output data. 
It can be the terminal or a file on the system (if you’ve enough privileges).

Stderr - Short for Standard error, It’s an output stream that the program writes error to. 
Similar to standard output, you can redirect the output error message to a file using 2> operator.

Pipes & Redirection
Pipes and redirections are used when you want to combine two or more commands together. 
For example, the first command can write a stream of output to a file, and the second command can operate on that file.

$ echo "Hello World" | tr [a-z] [A-Z]  
Output:
HELLO WORLD

In the above command we have taken the output of the first command and then passed it through the tr command which capitalized all the letters.


Redirection.

Most commonly, when we interact with Unix-like systems, the keyboard is the standard input and the terminal is the standard output. 
However, the file can also be the standard input or output. We can read from a file and use it as the input for a command. 
Similarly, We can also write the output of a command to a file. This is called redirection.
The following operators are used for redirection operations:

The > operator is used to redirect the output of a command to a file:

>_curl -L https://GitHub.com/earthly/earthly/raw/main/README.md > README.md

The above command read the content of the README.md file of the earthly repository and write its content to a README.md file on your local system.


The >> operator is used to append the output of a command to the end of an existing file or write to a new file.

For example, If you’ve a file already in your local system and you want to add new content to the end of file then you can use the >> operator:

$ cat numbers.txt
one
two
Say, you want to append “three” to this file. You can use the >> operator in this case:

echo "three" >> numbers.txt
If you use only the > then it will replace the content of the file with “three”.

The < operator is used to read from a file and then act on it. For example:

wc -l < numbers.txt > lines.txt
The above command takes the input of the wc line count command from the numbers.txt file and redirects the line count to a file called lines.txt


The 2> operator is used to redirect error messages to a particular file. 
This is useful when you’re executing a command or a script on your system and want to capture the error if something goes wrong:

$ kubectl apply -f namespace.yaml 2> error.txt

Using Both Together
You can redirect both stdout and stderr to separate or the same files:

Redirect stdout and stderr separately:

ls existing_dir non_existing_dir > output.txt 2> error.txt

output.txt: Stores successful command output.
error.txt: Stores error messages.

Redirect both stdout and stderr to the same file:

ls existing_dir non_existing_dir > all_output.txt 2>&1

2>&1 means "redirect stderr (2>) to where stdout (1>) is going".
all_output.txt will contain both normal output and errors.



Basic Linux Text Processing Commands

The cut Command
The cut command is used to cut out sections from each line of files and write the result to standard output. 
You can use it to extract a specific column from a file and print the column to the standard output.

$ gh gist list | cut -f 1
Output
8d1526fc09b5fc671994dbc0d4c7f8d7
5f15a60452277a5c7deb0c6b15a5a3f2
508bcea1782e7aed3b03f30fe053823a

In the command above, the gh gist list lists your gists and the cut -f 1 extracts the first field (i.e., the ID) from each line of the output. 
The fields are separated by whitespace.

The -f option is used to specify which fields to extract, and 1 specifies the first field.

You can also use the cut command to get a specific column from a CSV file.
$ cat demo.csv

name,age,language,hobby
john,21,golang,painting
jane,22,python,singing
jack,25,java,reading
jill,23,python,cooking


You can get the name field with the following command:
$ cut -d ',' -f 1 demo.csv

name
john
jane
jack
jill

The -d option stands for delimiter, and here the delimiter is a comma.

The cutcommand also supports selection of a range of fields. You can select from the second to the fourth field as shown below:

$ cut -d ',' -f 2-3 demo.csv

age,language,hobby
21,golang,painting
22,python,singing
25,java,reading
23,python,cooking


The sortCommand
The sort command is used to arrange lines in a text files in a specific order. By default, the sort command sorts the content of a file in ascending order.

$ cat names.txt

peter
ben
john

You can sort the content of this file by executing the following command:

$ sort names.txt

ben
john
peter

You can sort the content in descending order with the -r flag as shown below:
$ sort -r names.txt

Output:

peter
john
ben

There are other useful flags that you can use with the sort command. 
Some of the common flags include -f to ignore-case and the -o flag to write the output to a particular file.


The tac Command
The tac command prints the content of your file in the reverse order. 
If you’re familiar with the cat command, the tac command just returns the reverse of the output of the cat command.

$ cat numbers.txt
Output:
one
two
three
four
five

When you execute tac numbers.txt, it reverses the order of the content of the file as shown below:

tac numbers.txt
Output:

five
four
three
two 
one

A common use case of the tac command is in reading the content of log files. 
The tac command reverses the content of the log file. This shows the latest logs first.


The uniq Command
The uniq command is used to report or omit repeated lines. This command only print unique lines.

You can filter out the repeated line by using the uniq command:

$ uniq names.txt
Output:

Outputjohn
jane
john
jane
john
The uniq command will remove only the adjacent repeated lines. In this case, there are no adjacent repeated lines, so the output will be the same as the input.

To remove all repeated lines from the file, you can use the -u option with the sort command like this:

>_$ sort names.txt | uniq -u
This will sort the file contents and then remove all non-unique lines. The output will be empty because all lines in the original file were repeated.

The uniq command considers a line to be duplicate if they are adjacent to each other.

To understand the uniq command better, let’s change the content of the above file. Let’s create a temp.txt file and add the following content in it:

>_$ cat temp.txt
Output:


jane
jane
john
john
john 
When can execute the uniq command on this new file:

>_$ uniq temp.txt 
Output:

jane
john
Since the file had repeated lines, the uniq command only removed the adjacent repeated lines and left the unique lines intact.

You can also use -c flag to count the number of occurrence of each line:

>_$ uniq -c temp.txt
Output:

2 jane
3 john
This command is helpful when you want to remove the duplicate entries/lines from a file.


The sedCommand
The [sed] is used to perform various operations on text streams, including search-and-replace, text filtering, and line numbering. 
Let consider this example that search and substitutes k8s with kubernetes in a blog content.
$ sed 's/k8s/kubernetes/g' blog.md

In the above case, s means that we want to substitute a word, k8s is the word we want to substitute and kubernetes is the word that we want to substitute for k8s.
g in the command means that we want this command to be executed in global scope which means that every instance of the match will be replaced.

If you call the command without the g, only the first instance of k8s will be replaced by kubernetes:


The awk Command
The awk command is a versatile tool that can help you in text processing, searching for patterns in a file, scripting etc.

With the help of awk, you can do very advanced text processing like summing up numbers in a column of a file, Filtering based on a condition, Grouping and aggregating data etc . 
In this tutorial, you will use the awk command to select and sum the columns from the fields of a CSV file and also use it to filter out the content of a file.

The CSV file has a name, age, language, and hobby fields. You will first retrieve the age field, and then perform the sum operation on it.

The content of the CSV file is as shown below:

>_cat demo.csv
Output:

name,age,language,hobby
john,21,golang,painting
jane,22,python,singing
jack,25,java,reading
jill,23,python,cooking
jake,12,rust,hiking

To get the age field which is the second field:

>_$ awk -F , '{print $2}' demo.csv

Output:

age
21
22
25
23
12
The -F flag in the above command sets the field separator to , which means awk will treat the file as a comma-separated value. 
With {print $2}, awk goes through each line of the file and prints the second field with.

To compute the sum of the ages in the file:

>_$ awk -F , '{sum+=$2} END {print sum}' demo.csv
Output:

103

The above command calculates the sum of the second field in the file. 
The {sum+=2} will look at the second value in every field and then add it to the sum variable. 
The END {print sum} will be executed when all the lines are processed and prints out the final value of the sum variable.


While performing numerical computation on the content of a file, you can ignore the first line of the file in case it is a heading which are most cases,
strings:

>_$ awk -F , 'NR>1 {print $2}' demo.csv
Output:

21
22
25
23
12
NR refers to the number of records. NR>1 in the above examples means that it will not include the first record, which is the heading and not numeric data (age).


You can also use awk to filter out the content of a file. Here’s an awk script that will filter out all the links from your markdown document:

>_awk 'match($0, /\[([^\]]*)\]\(([^\)]*)\)/, a) \
{print "["a[1]"] ==> ("a[2]")"}' file.md
Output:

# truncated
[cut] ==> (https://man7.org/linux/man-pages/man1/cut.1.html)
[sort] ==> (https://man7.org/linux/man-pages/man1/sort.1.html)
[tac] ==> (https://man7.org/linux/man-pages/man1/tac.1.html)
[uniq] ==> (https://man7.org/linux/man-pages/man1/uniq.1.html)

The above command retrieves all the links from a markdown document. The link in a standard Markdown looks as shown below:

Output[Earthly](https://earthly.dev/)
So,the above command retrieves the link by looking for the pattern that matches how a link is embedded in a standard markdown. 
You can further use this to check if links are accessible or not.



The tr Command
The tr command can be used to translate or delete characters from standard input and write the result to standard output. 
Some of the common use cases of the tr command include:

Translating characters from one set to another: For example, you can use the command to translate all the characters from uppercase to lowercase and vice-versa.

Removing repeated characters: You can use the command to delete and remove repeating characters from a stream of text.

Deleting characters: You can use them to delete specific characters from the input. For example, you can use the tr command to delete all digits from the input.

The command can be really helpful if you’re performing an operation that requires small letters only, then you can use this command as shown below:
$ echo "Hello World" | tr [A-Z] [a-z]

And it will return hello world.

You can also use the following syntax for the same purpose.
>_echo "Hello" | tr [:upper] [:lower]

Here is a practical example that you might encounter. 
Let’s say you have a project called “CoolProject” on GitHub and you are using a prebuilt workflow that relies on the $ variable as a tag for your container image.
When the workflow is triggered, this variable will be replaced with the name of your repository. 
However, if your repository name includes uppercase letters, you may encounter an error similar to this:


OutputError parsing reference: "ghcr.io/username/CoolProject" \
is not a valid repository/tag: invalid reference format: \
repository name must be lowercase


To avoid this error, you can use the tr command to convert the repository name to lowercase and use the result as the tag for your image. 
You can achieve this by passing $ to the tr command as shown below:

echo $ | tr '[:upper:]' '[:lower:]'

This will ensure that your repository name is always in lowercase, which is required for container image tags.


